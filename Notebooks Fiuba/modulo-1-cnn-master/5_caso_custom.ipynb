{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40baf710-715f-426e-b006-14768986e4e5",
   "metadata": {},
   "source": [
    "# Custom Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e690b236-9cc3-40f9-b350-1b0c349100b7",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datamunge/sign-language-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6410b50b-2063-487f-9431-ef570229800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from __future__ import print_function\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941bcb0-ada0-42e3-ba38-7b7620905cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "signs = {'0': 'A', '1': 'B', '2': 'C', '3': 'D', '4': 'E', '5': 'F', \n",
    "         '6': 'G', '7': 'H', '8': 'I', '10': 'K', '11': 'L', '12': 'M', \n",
    "         '13': 'N', '14': 'O', '15': 'P', '16': 'Q', '17': 'R', '18': 'S', \n",
    "         '19': 'T', '20': 'U', '21': 'V', '22': 'W', '23': 'X', '24': 'Y' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c57c2d7-9797-47c1-bd02-9787ff71a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "signs_lang_dataset = pd.read_csv('dataset/sign_mnist_train/sign_mnist_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8277c4-a6b0-4c45-a207-9cc00302e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignsLanguageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, train = True):\n",
    "        \n",
    "        self.train = train\n",
    "        \n",
    "        if self.train == True:\n",
    "            self.signs_lang_dataset = pd.read_csv('dataset/sign_mnist_train/sign_mnist_train.csv')\n",
    "        else:\n",
    "            self.signs_lang_dataset = pd.read_csv('dataset/sign_mnist_test/sign_mnist_test.csv')\n",
    "            \n",
    "        self.X_set = self.signs_lang_dataset.iloc[:, 1:].values\n",
    "        self.y_set = self.signs_lang_dataset.iloc[:, 0].values\n",
    "        \n",
    "        self.X_set = np.reshape(self.X_set, (self.X_set.shape[0], 1, 28, 28)) / 255\n",
    "        self.y_set = np.array(self.y_set)\n",
    "            \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = self.X_set[index, :, :]\n",
    "        \n",
    "        label = self.y_set[index]\n",
    "        \n",
    "        sample = {'image_sign': image, 'label': label}\n",
    "        \n",
    "        return sample\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X_set.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf465c-4c61-44b0-8f8b-14ad065eec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 40, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(40, 20, kernel_size = 5)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm2d(40)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(20)\n",
    "        \n",
    "        self.fc1 = nn.Linear(320, 100)\n",
    "        self.fc2 = nn.Linear(100, 25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a92f7a7-0d28-4d3e-aeea-d56882ccc29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epoch, device, train_loader, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        \n",
    "        img = data['image_sign']\n",
    "        img = img.type(torch.FloatTensor).to(device)\n",
    "        target = data['label']\n",
    "        target = target.type(torch.LongTensor).to(device)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(img)\n",
    "\n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(img), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            \n",
    "            img = data['image_sign']\n",
    "            img = img.type(torch.FloatTensor).to(device)\n",
    "            target = data['label']\n",
    "            target = target.type(torch.LongTensor).to(device)\n",
    "            \n",
    "            output = model(img)\n",
    "            test_loss += F.nll_loss(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f61b50a-b59a-4d75-9b7e-b0f52412457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 5\n",
    "batch_size_test = 4\n",
    "\n",
    "dataset_train = SignsLanguageDataset(train = True)\n",
    "dataset_test = SignsLanguageDataset(train = False)\n",
    "train_loader = DataLoader(dataset = dataset_train, batch_size = batch_size_train)\n",
    "test_loader = DataLoader(dataset = dataset_test, batch_size = batch_size_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2601948-ea44-47e9-b09a-4a2a6f20886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "use_cuda = False\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 7\n",
    "model = Net()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.2, weight_decay = 0.002)\n",
    "\n",
    "log_interval = 27455\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36c2f7-ce16-4e05-bd52-2b63bcb0baf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, optimizer, epoch, device, train_loader, log_interval)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8105ab0-d699-48d1-9be2-38e850a11b21",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799ce68-70cf-460a-9568-3197175ce52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_trained.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090716ea-a185-4d5d-b81f-f55969ce0f5b",
   "metadata": {},
   "source": [
    "# Predicci√≥n en real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e96836-3c73-4dc6-9110-fd8720b2d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a23aafa-fd58-4973-990f-e108f9e7a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "cap.set(3, 700)\n",
    "cap.set(4, 480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7612536-b8ff-4ccf-989f-87d9c9023957",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = torch.load('model_trained.pt')\n",
    "modelo.eval()\n",
    "\n",
    "signs = {'0': 'A', '1': 'B', '2': 'C', '3': 'D', '4': 'E', '5': 'F', '6': 'G', '7': 'H', '8': 'I',\n",
    "        '10': 'K', '11': 'L', '12': 'M', '13': 'N', '14': 'O', '15': 'P', '16': 'Q', '17': 'R',\n",
    "        '18': 'S', '19': 'T', '20': 'U', '21': 'V', '22': 'W', '23': 'X', '24': 'Y' }\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Lugar de la imagen donde se toma la muestra\n",
    "    img = frame[20:250, 20:250]\n",
    "\n",
    "    res = cv2.resize(img, dsize=(28, 28), interpolation = cv2.INTER_CUBIC)\n",
    "    res = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    res1 = np.reshape(res, (1, 1, 28, 28)) / 255\n",
    "    res1 = torch.from_numpy(res1)\n",
    "    res1 = res1.type(torch.FloatTensor)\n",
    "\n",
    "    out = modelo(res1)\n",
    "    # Probabilidades\n",
    "    probs, label = torch.topk(out, 25)\n",
    "    probs = torch.nn.functional.softmax(probs, 1)\n",
    "\n",
    "    pred = out.max(1, keepdim=True)[1]\n",
    "\n",
    "    if float(probs[0,0]) < 0.5:\n",
    "        texto_mostrar = 'Signo no detectado'\n",
    "    else:\n",
    "        texto_mostrar = signs[str(int(pred))] + ': ' + '{:.2f}'.format(float(probs[0,0])) + '%'\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    frame = cv2.putText(frame, texto_mostrar, (60,285), font, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "\n",
    "    frame = cv2.rectangle(frame, (20, 20), (250, 250), (0, 255, 0), 3)\n",
    "\n",
    "    cv2.imshow('Cam', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
