{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "name": "autor.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0l30N2uV1del",
        "outputId": "2984b3f9-dd0a-4e4a-8588-1c50ab3cd8be"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab Notebooks"
      ],
      "id": "0l30N2uV1del",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SRiXqaJ12ys",
        "outputId": "c3421f2d-f3e0-40a5-bf8e-ae678ab924b1"
      },
      "source": [
        "!pip  install hdf5storage"
      ],
      "id": "0SRiXqaJ12ys",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hdf5storage in /usr/local/lib/python3.7/dist-packages (0.1.18)\n",
            "Requirement already satisfied: h5py>=2.1 in /usr/local/lib/python3.7/dist-packages (from hdf5storage) (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hdf5storage) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.1->hdf5storage) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e260a809"
      },
      "source": [
        "This is a brief code on how to use a GNN in the authorship attribution problem."
      ],
      "id": "e260a809"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83ef8916"
      },
      "source": [
        "# First, we import everything we need to import\n",
        "\n",
        "# Standard libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import torch; torch.set_default_dtype(torch.float64)\n",
        "import torch.nn as nn\n",
        "\n",
        "# Own libraries\n",
        "import Utils.graphML as gml # This one has the GNN layers\n",
        "import Utils.dataTools as dataTools # This one has the dataset"
      ],
      "id": "83ef8916",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cbd3324"
      },
      "source": [
        "# Data\n",
        "\n",
        "Next, we have to load the data"
      ],
      "id": "3cbd3324"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1df8b994"
      },
      "source": [
        "# Determine the basic parameters\n",
        "authorName = 'austen'\n",
        "# jacob 'abbott',         robert louis 'stevenson',   louisa may 'alcott',\n",
        "# horatio 'alger',        james 'allen',              jane 'austen',\n",
        "# emily 'bronte',         james 'cooper',             charles 'dickens',\n",
        "# hamlin 'garland',       nathaniel 'hawthorne',      henry 'james',\n",
        "# herman 'melville',      thomas nelson 'page',       henry 'thoreau',\n",
        "# mark 'twain',           arthur conan 'doyle',       washington 'irving',\n",
        "# edgar allan 'poe',      sarah orne 'jewett',        edith 'wharton'\n",
        "\n",
        "ratioTrain = 0.95 # Ratio of signals that will be used for training\n",
        "ratioValid = 0.08 # Ratio of signals in the training set that will go for validation\n",
        "dataPath = os.path.join('dataset','authorshipData.mat') # Where the data is located"
      ],
      "id": "1df8b994",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cc25b2e"
      },
      "source": [
        "# Load the data\n",
        "\n",
        "data = dataTools.Authorship(authorName,\n",
        "                            ratioTrain,\n",
        "                            ratioValid,\n",
        "                            dataPath,\n",
        "                            # Leave all this as-is, they are details on how the graph is built\n",
        "                            'rows', # Normalize the adjacency matrix by 'rows' or 'columns'\n",
        "                            False, # when False, if the graph has isolated nodes, discard them\n",
        "                            True, # Create an undirected graph (symmetrize directed edges)\n",
        "                            True) # Ensure the resulting graph is connected"
      ],
      "id": "9cc25b2e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cadf8f78",
        "outputId": "bfa54389-1f55-440a-8c8e-31bd78fafda0"
      },
      "source": [
        "# Get important information from the loaded data\n",
        "\n",
        "S = data.getGraph() # Get the adjacency matrix\n",
        "N = S.shape[0] # Get the number of nodes\n",
        "xTrain, yTrain = data.getSamples('train') # Get the training samples\n",
        "xValid, yValid = data.getSamples('valid') # Get the validation samples\n",
        "xTest, yTest = data.getSamples('test') # Get the test samples\n",
        "nTrain = xTrain.shape[0] # Number of training samples\n",
        "nValid = xValid.shape[0] # Number of validation samples\n",
        "nTest = xTest.shape[0] # Number of testing samples\n",
        "\n",
        "# Print some info\n",
        "print(\"Number of nodes: %d\" % N)\n",
        "print(\"Number of training signals: %d\" % nTrain)\n",
        "print(\"Number of validation signals: %d\" % nValid)\n",
        "print(\"Number of testing signals: %d\" % nTest)\n",
        "\n",
        "# Carry out several important adaptations\n",
        "\n",
        "# Normalize the adjacency matrix\n",
        "S = S/np.max(np.linalg.eigvals(S))\n",
        "\n",
        "# Add the extra \"edge_feature\" dimension to the matrix\n",
        "S = np.expand_dims(S, axis = 0) # 1 x N x N\n",
        "\n",
        "# Note that the xTrain, xValid and xTest are of shape B x N, \n",
        "# but we want them to be B x F x N with F = 1 (there's only one input feature)\n",
        "xTrain = np.expand_dims(xTrain, axis = 1)\n",
        "xValid = np.expand_dims(xValid, axis = 1)\n",
        "xTest = np.expand_dims(xTest, axis = 1)"
      ],
      "id": "cadf8f78",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 189\n",
            "Number of training signals: 1346\n",
            "Number of validation signals: 118\n",
            "Number of testing signals: 78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e6a3399",
        "outputId": "8a72734a-3e23-4f9c-a5e4-8b75e3410c3f"
      },
      "source": [
        "# Double-check everything is the way it's supposed to be:\n",
        "print(\"Spectral norm of S = %.4f\" % np.linalg.norm(S[0], ord = 2))\n",
        "print(\"Shape of S: \", S.shape)\n",
        "print(\"Shape of xTrain: \", xTrain.shape)\n",
        "print(\"Shape of xValid: \", xValid.shape)\n",
        "print(\"Shape of xTest: \", xTest.shape)"
      ],
      "id": "5e6a3399",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spectral norm of S = 1.0000\n",
            "Shape of S:  (1, 189, 189)\n",
            "Shape of xTrain:  (1346, 1, 189)\n",
            "Shape of xValid:  (118, 1, 189)\n",
            "Shape of xTest:  (78, 1, 189)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d3d363b"
      },
      "source": [
        "# Architecture\n",
        "\n",
        "Let's create the architecture"
      ],
      "id": "4d3d363b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70391672"
      },
      "source": [
        "# We start with a 2-layer GNN\n",
        "class GNN2Ly(nn.Module):\n",
        "    \n",
        "    def __init__(self, F1, F2, K1, K2, S):\n",
        "        # F1: Number of features (hidden units) at the output of the first layer\n",
        "        # F2: Number of features (hidden units) at the output of the second layer\n",
        "        # K1: Number of K-hops to consider in the first layer\n",
        "        # K2: Number of K-hops to consider in the second layer\n",
        "        # S: Graph matrix description\n",
        "        \n",
        "        # Initialize the parent\n",
        "        super().__init__()\n",
        "        \n",
        "        # First layer\n",
        "        self.graphConvLy1 = gml.GraphConv(1, F1, K1) # The first '1' is because the input feature is 1\n",
        "        self.graphConvLy1.set_graph(S) # Set the graph we're going to use\n",
        "        # Nonlinear activation function\n",
        "        self.activationFunction1 = nn.ReLU()\n",
        "        \n",
        "        # Second layer\n",
        "        self.graphConvLy2 = gml.GraphConv(F1, F2, K2) # F1 input features from previous layer\n",
        "        self.graphConvLy2.set_graph(S) # Set the graph we're going to use\n",
        "        # Nonlinear activation function\n",
        "        self.activationFunction2 = nn.ReLU()\n",
        "        \n",
        "        # Readout layer\n",
        "        self.readoutLayer = nn.Linear(F2 * S.shape[1], 2)\n",
        "        #    Note that at the end of the second layer we have N = S.shape[1] nodes, each one with F2 features\n",
        "        #    Therefore, we want to flatten all of this into a single vector to pass it to the readout layer\n",
        "        #    The output of the readout layer is the number of classes (or it could be just 1, depending on\n",
        "        #    what function we're going to use to train this)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # Remember the signal has shape B x Fin x N, where Fin = 1 for us\n",
        "        \n",
        "        # Apply the first layer\n",
        "        y = self.graphConvLy1(x) # Output has shape B x F1 x N\n",
        "        y = self.activationFunction1(y) # Nonlinear activation\n",
        "        \n",
        "        # Apply the second layer\n",
        "        y = self.graphConvLy2(y) # Output has shape B x F2 x N\n",
        "        y = self.activationFunction2(y) # Nonlinear activation\n",
        "        \n",
        "        # Flatten the output before moving to apply the readout layer\n",
        "        y = y.reshape(y.shape[0], y.shape[1]*y.shape[2])\n",
        "        \n",
        "        # Apply the readout layer\n",
        "        y = self.readoutLayer(y)\n",
        "        \n",
        "        return y"
      ],
      "id": "70391672",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bf08c75"
      },
      "source": [
        "# Set the parameters\n",
        "F1 = 32 # Number of features at the output of the first layer\n",
        "F2 = 64 # Number of features at the output of the second layer\n",
        "K1 = 3 # Gather information up to the 3-hop neighborhood\n",
        "K2 = 2 # Gather information up to the 2-hop neighborhood\n",
        "S = torch.tensor(S) # Convert the graph into a torch.tensor before passing it to the architecture\n",
        "\n",
        "GNN = GNN2Ly(F1, F2, K1, K2, S ) # Don't forget to add the graph"
      ],
      "id": "3bf08c75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb12044c"
      },
      "source": [
        "# Training\n",
        "\n",
        "We're going to do some training here, as usual."
      ],
      "id": "bb12044c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2775450"
      },
      "source": [
        "nEpochs = 25 # Number of epochs\n",
        "batchSize = 20 # Batch size\n",
        "learningRate = 0.005 # Learning rate for an ADAM optimizer\n",
        "lossFunction = nn.CrossEntropyLoss() # Loss function to use\n",
        "optimizer = torch.optim.Adam(GNN.parameters(), lr = learningRate)"
      ],
      "id": "b2775450",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74de4c92",
        "outputId": "08bb689e-ca4e-4844-e10f-bfebcbe2dec1"
      },
      "source": [
        "# For each epoch\n",
        "for e in range(nEpochs):\n",
        "    # Shuffle the batch indices\n",
        "    idxRandom = np.random.permutation(nTrain)\n",
        "    # Number of batches\n",
        "    nBatches = (nTrain//batchSize) if np.mod(nTrain,batchSize) == 0 else (nTrain//batchSize + 1)\n",
        "    \n",
        "    # For each batch\n",
        "    for b in range(nBatches):\n",
        "        # Get the data\n",
        "        xBatch = xTrain[b*batchSize : np.min(((b+1)*batchSize,nTrain)), :, :]\n",
        "        yBatch = yTrain[b*batchSize : np.min(((b+1)*batchSize,nTrain))]\n",
        "        # Convert it to tensor\n",
        "        xBatch = torch.tensor(xBatch)\n",
        "        yBatch = torch.tensor(yBatch)\n",
        "\n",
        "        # Reset gradients\n",
        "        GNN.zero_grad()\n",
        "\n",
        "        # Compute the output\n",
        "        yHat = GNN(xBatch)\n",
        "\n",
        "        # Compute the loss\n",
        "        lossValue = lossFunction(yHat, yBatch)\n",
        "\n",
        "        # Compute the gradient\n",
        "        lossValue.backward()\n",
        "\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print the info\n",
        "        if np.mod(e+b,5) == 0:\n",
        "            print(\"E: %3d, B: %3d, loss = %.4f\" % (e, b, lossValue.item()))"
      ],
      "id": "74de4c92",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E:   0, B:   0, loss = 0.8261\n",
            "E:   0, B:   5, loss = 0.0000\n",
            "E:   0, B:  10, loss = 0.0000\n",
            "E:   0, B:  15, loss = 0.0000\n",
            "E:   0, B:  20, loss = 0.0000\n",
            "E:   0, B:  25, loss = 0.0000\n",
            "E:   0, B:  30, loss = 0.0000\n",
            "E:   0, B:  35, loss = 313.5316\n",
            "E:   0, B:  40, loss = 0.0000\n",
            "E:   0, B:  45, loss = 0.0000\n",
            "E:   0, B:  50, loss = 0.0000\n",
            "E:   0, B:  55, loss = 0.0000\n",
            "E:   0, B:  60, loss = 0.0000\n",
            "E:   0, B:  65, loss = 0.0000\n",
            "E:   1, B:   4, loss = 148.7365\n",
            "E:   1, B:   9, loss = 0.1100\n",
            "E:   1, B:  14, loss = 0.0000\n",
            "E:   1, B:  19, loss = 0.0000\n",
            "E:   1, B:  24, loss = 0.0000\n",
            "E:   1, B:  29, loss = 0.0000\n",
            "E:   1, B:  34, loss = 81.3120\n",
            "E:   1, B:  39, loss = 9.7011\n",
            "E:   1, B:  44, loss = 0.0000\n",
            "E:   1, B:  49, loss = 0.0000\n",
            "E:   1, B:  54, loss = 0.0000\n",
            "E:   1, B:  59, loss = 0.0000\n",
            "E:   1, B:  64, loss = 0.0000\n",
            "E:   2, B:   3, loss = 48.6574\n",
            "E:   2, B:   8, loss = 0.0081\n",
            "E:   2, B:  13, loss = 0.0000\n",
            "E:   2, B:  18, loss = 0.0000\n",
            "E:   2, B:  23, loss = 0.0000\n",
            "E:   2, B:  28, loss = 0.0000\n",
            "E:   2, B:  33, loss = 22.1988\n",
            "E:   2, B:  38, loss = 34.1977\n",
            "E:   2, B:  43, loss = 5.7522\n",
            "E:   2, B:  48, loss = 0.0000\n",
            "E:   2, B:  53, loss = 0.0000\n",
            "E:   2, B:  58, loss = 0.0000\n",
            "E:   2, B:  63, loss = 0.0000\n",
            "E:   3, B:   2, loss = 25.6620\n",
            "E:   3, B:   7, loss = 12.2008\n",
            "E:   3, B:  12, loss = 3.9424\n",
            "E:   3, B:  17, loss = 1.6291\n",
            "E:   3, B:  22, loss = 0.8282\n",
            "E:   3, B:  27, loss = 0.5951\n",
            "E:   3, B:  32, loss = 0.5274\n",
            "E:   3, B:  37, loss = 0.9154\n",
            "E:   3, B:  42, loss = 0.8965\n",
            "E:   3, B:  47, loss = 0.8798\n",
            "E:   3, B:  52, loss = 0.8505\n",
            "E:   3, B:  57, loss = 0.8152\n",
            "E:   3, B:  62, loss = 0.7872\n",
            "E:   3, B:  67, loss = 0.7601\n",
            "E:   4, B:   1, loss = 0.6269\n",
            "E:   4, B:   6, loss = 0.6345\n",
            "E:   4, B:  11, loss = 0.6309\n",
            "E:   4, B:  16, loss = 0.6213\n",
            "E:   4, B:  21, loss = 0.6071\n",
            "E:   4, B:  26, loss = 0.5915\n",
            "E:   4, B:  31, loss = 0.5723\n",
            "E:   4, B:  36, loss = 0.8318\n",
            "E:   4, B:  41, loss = 0.8457\n",
            "E:   4, B:  46, loss = 0.8153\n",
            "E:   4, B:  51, loss = 0.7718\n",
            "E:   4, B:  56, loss = 0.7338\n",
            "E:   4, B:  61, loss = 0.6849\n",
            "E:   4, B:  66, loss = 0.6178\n",
            "E:   5, B:   0, loss = 0.6643\n",
            "E:   5, B:   5, loss = 0.6946\n",
            "E:   5, B:  10, loss = 0.7228\n",
            "E:   5, B:  15, loss = 0.6681\n",
            "E:   5, B:  20, loss = 0.6514\n",
            "E:   5, B:  25, loss = 0.6206\n",
            "E:   5, B:  30, loss = 0.5924\n",
            "E:   5, B:  35, loss = 0.6492\n",
            "E:   5, B:  40, loss = 0.7350\n",
            "E:   5, B:  45, loss = 0.6488\n",
            "E:   5, B:  50, loss = 0.5492\n",
            "E:   5, B:  55, loss = 0.4046\n",
            "E:   5, B:  60, loss = 0.2943\n",
            "E:   5, B:  65, loss = 0.0114\n",
            "E:   6, B:   4, loss = 2.6503\n",
            "E:   6, B:   9, loss = 0.6295\n",
            "E:   6, B:  14, loss = 0.5945\n",
            "E:   6, B:  19, loss = 0.5765\n",
            "E:   6, B:  24, loss = 0.5392\n",
            "E:   6, B:  29, loss = 0.5388\n",
            "E:   6, B:  34, loss = 0.8289\n",
            "E:   6, B:  39, loss = 0.7628\n",
            "E:   6, B:  44, loss = 0.6225\n",
            "E:   6, B:  49, loss = 0.6880\n",
            "E:   6, B:  54, loss = 0.4400\n",
            "E:   6, B:  59, loss = 0.0910\n",
            "E:   6, B:  64, loss = 0.0200\n",
            "E:   7, B:   3, loss = 0.7052\n",
            "E:   7, B:   8, loss = 0.0692\n",
            "E:   7, B:  13, loss = 0.0004\n",
            "E:   7, B:  18, loss = 0.0000\n",
            "E:   7, B:  23, loss = 0.0000\n",
            "E:   7, B:  28, loss = 0.0000\n",
            "E:   7, B:  33, loss = 6.5443\n",
            "E:   7, B:  38, loss = 2.1781\n",
            "E:   7, B:  43, loss = 0.0800\n",
            "E:   7, B:  48, loss = 0.0046\n",
            "E:   7, B:  53, loss = 0.0106\n",
            "E:   7, B:  58, loss = 0.0162\n",
            "E:   7, B:  63, loss = 0.0095\n",
            "E:   8, B:   2, loss = 2.7176\n",
            "E:   8, B:   7, loss = 0.5812\n",
            "E:   8, B:  12, loss = 0.5575\n",
            "E:   8, B:  17, loss = 0.5349\n",
            "E:   8, B:  22, loss = 0.5227\n",
            "E:   8, B:  27, loss = 0.4739\n",
            "E:   8, B:  32, loss = 0.4846\n",
            "E:   8, B:  37, loss = 0.8650\n",
            "E:   8, B:  42, loss = 0.7915\n",
            "E:   8, B:  47, loss = 0.7625\n",
            "E:   8, B:  52, loss = 0.2017\n",
            "E:   8, B:  57, loss = 0.0004\n",
            "E:   8, B:  62, loss = 0.0000\n",
            "E:   8, B:  67, loss = 0.0000\n",
            "E:   9, B:   1, loss = 20.7179\n",
            "E:   9, B:   6, loss = 7.1128\n",
            "E:   9, B:  11, loss = 0.8010\n",
            "E:   9, B:  16, loss = 0.4094\n",
            "E:   9, B:  21, loss = 0.4237\n",
            "E:   9, B:  26, loss = 0.4794\n",
            "E:   9, B:  31, loss = 0.3924\n",
            "E:   9, B:  36, loss = 0.8208\n",
            "E:   9, B:  41, loss = 0.9215\n",
            "E:   9, B:  46, loss = 0.1084\n",
            "E:   9, B:  51, loss = 0.0000\n",
            "E:   9, B:  56, loss = 0.0000\n",
            "E:   9, B:  61, loss = 0.0000\n",
            "E:   9, B:  66, loss = 0.0000\n",
            "E:  10, B:   0, loss = 28.7707\n",
            "E:  10, B:   5, loss = 13.9664\n",
            "E:  10, B:  10, loss = 2.6433\n",
            "E:  10, B:  15, loss = 0.3849\n",
            "E:  10, B:  20, loss = 0.3578\n",
            "E:  10, B:  25, loss = 0.3372\n",
            "E:  10, B:  30, loss = 0.3317\n",
            "E:  10, B:  35, loss = 0.9477\n",
            "E:  10, B:  40, loss = 1.0923\n",
            "E:  10, B:  45, loss = 0.8628\n",
            "E:  10, B:  50, loss = 0.7070\n",
            "E:  10, B:  55, loss = 0.5082\n",
            "E:  10, B:  60, loss = 0.4313\n",
            "E:  10, B:  65, loss = 0.5273\n",
            "E:  11, B:   4, loss = 0.3489\n",
            "E:  11, B:   9, loss = 0.4894\n",
            "E:  11, B:  14, loss = 0.3650\n",
            "E:  11, B:  19, loss = 0.4445\n",
            "E:  11, B:  24, loss = 0.3598\n",
            "E:  11, B:  29, loss = 0.3584\n",
            "E:  11, B:  34, loss = 0.9804\n",
            "E:  11, B:  39, loss = 0.7480\n",
            "E:  11, B:  44, loss = 0.5906\n",
            "E:  11, B:  49, loss = 0.9169\n",
            "E:  11, B:  54, loss = 0.5196\n",
            "E:  11, B:  59, loss = 0.2943\n",
            "E:  11, B:  64, loss = 0.7324\n",
            "E:  12, B:   3, loss = 0.5537\n",
            "E:  12, B:   8, loss = 0.4093\n",
            "E:  12, B:  13, loss = 0.3669\n",
            "E:  12, B:  18, loss = 0.4245\n",
            "E:  12, B:  23, loss = 0.3653\n",
            "E:  12, B:  28, loss = 0.3617\n",
            "E:  12, B:  33, loss = 0.5541\n",
            "E:  12, B:  38, loss = 0.8472\n",
            "E:  12, B:  43, loss = 0.6100\n",
            "E:  12, B:  48, loss = 0.6325\n",
            "E:  12, B:  53, loss = 0.4097\n",
            "E:  12, B:  58, loss = 0.4251\n",
            "E:  12, B:  63, loss = 0.3801\n",
            "E:  13, B:   2, loss = 2.0144\n",
            "E:  13, B:   7, loss = 0.5384\n",
            "E:  13, B:  12, loss = 0.3438\n",
            "E:  13, B:  17, loss = 0.3793\n",
            "E:  13, B:  22, loss = 0.1062\n",
            "E:  13, B:  27, loss = 0.0055\n",
            "E:  13, B:  32, loss = 0.0009\n",
            "E:  13, B:  37, loss = 0.6296\n",
            "E:  13, B:  42, loss = 0.7078\n",
            "E:  13, B:  47, loss = 0.2315\n",
            "E:  13, B:  52, loss = 0.0000\n",
            "E:  13, B:  57, loss = 0.0000\n",
            "E:  13, B:  62, loss = 0.0000\n",
            "E:  13, B:  67, loss = 0.0000\n",
            "E:  14, B:   1, loss = 86.1618\n",
            "E:  14, B:   6, loss = 0.0052\n",
            "E:  14, B:  11, loss = 0.2141\n",
            "E:  14, B:  16, loss = 0.1757\n",
            "E:  14, B:  21, loss = 0.0703\n",
            "E:  14, B:  26, loss = 0.0243\n",
            "E:  14, B:  31, loss = 0.0100\n",
            "E:  14, B:  36, loss = 3.8977\n",
            "E:  14, B:  41, loss = 1.4433\n",
            "E:  14, B:  46, loss = 0.6779\n",
            "E:  14, B:  51, loss = 0.3472\n",
            "E:  14, B:  56, loss = 0.2980\n",
            "E:  14, B:  61, loss = 0.1634\n",
            "E:  14, B:  66, loss = 0.2868\n",
            "E:  15, B:   0, loss = 0.6485\n",
            "E:  15, B:   5, loss = 0.7021\n",
            "E:  15, B:  10, loss = 0.6909\n",
            "E:  15, B:  15, loss = 0.4749\n",
            "E:  15, B:  20, loss = 0.4500\n",
            "E:  15, B:  25, loss = 0.4288\n",
            "E:  15, B:  30, loss = 0.4233\n",
            "E:  15, B:  35, loss = 0.8309\n",
            "E:  15, B:  40, loss = 1.0102\n",
            "E:  15, B:  45, loss = 0.8514\n",
            "E:  15, B:  50, loss = 0.7765\n",
            "E:  15, B:  55, loss = 0.6392\n",
            "E:  15, B:  60, loss = 0.5899\n",
            "E:  15, B:  65, loss = 0.5690\n",
            "E:  16, B:   4, loss = 0.4744\n",
            "E:  16, B:   9, loss = 0.5529\n",
            "E:  16, B:  14, loss = 0.4796\n",
            "E:  16, B:  19, loss = 0.6145\n",
            "E:  16, B:  24, loss = 0.4884\n",
            "E:  16, B:  29, loss = 0.4536\n",
            "E:  16, B:  34, loss = 0.7483\n",
            "E:  16, B:  39, loss = 0.4821\n",
            "E:  16, B:  44, loss = 0.4983\n",
            "E:  16, B:  49, loss = 0.7314\n",
            "E:  16, B:  54, loss = 0.4714\n",
            "E:  16, B:  59, loss = 0.3496\n",
            "E:  16, B:  64, loss = 0.6759\n",
            "E:  17, B:   3, loss = 0.5384\n",
            "E:  17, B:   8, loss = 0.4233\n",
            "E:  17, B:  13, loss = 0.4144\n",
            "E:  17, B:  18, loss = 0.5983\n",
            "E:  17, B:  23, loss = 0.3870\n",
            "E:  17, B:  28, loss = 0.4778\n",
            "E:  17, B:  33, loss = 0.6809\n",
            "E:  17, B:  38, loss = 0.4199\n",
            "E:  17, B:  43, loss = 0.3688\n",
            "E:  17, B:  48, loss = 0.5337\n",
            "E:  17, B:  53, loss = 0.4093\n",
            "E:  17, B:  58, loss = 0.4086\n",
            "E:  17, B:  63, loss = 0.4549\n",
            "E:  18, B:   2, loss = 0.6792\n",
            "E:  18, B:   7, loss = 0.5143\n",
            "E:  18, B:  12, loss = 0.4172\n",
            "E:  18, B:  17, loss = 0.6382\n",
            "E:  18, B:  22, loss = 0.4221\n",
            "E:  18, B:  27, loss = 0.5113\n",
            "E:  18, B:  32, loss = 0.4008\n",
            "E:  18, B:  37, loss = 0.6732\n",
            "E:  18, B:  42, loss = 0.6944\n",
            "E:  18, B:  47, loss = 0.7030\n",
            "E:  18, B:  52, loss = 0.4863\n",
            "E:  18, B:  57, loss = 0.4807\n",
            "E:  18, B:  62, loss = 0.6486\n",
            "E:  18, B:  67, loss = 0.1223\n",
            "E:  19, B:   1, loss = 0.6648\n",
            "E:  19, B:   6, loss = 0.5144\n",
            "E:  19, B:  11, loss = 0.4404\n",
            "E:  19, B:  16, loss = 0.4364\n",
            "E:  19, B:  21, loss = 0.5533\n",
            "E:  19, B:  26, loss = 0.7137\n",
            "E:  19, B:  31, loss = 0.5043\n",
            "E:  19, B:  36, loss = 0.6478\n",
            "E:  19, B:  41, loss = 0.8207\n",
            "E:  19, B:  46, loss = 0.6223\n",
            "E:  19, B:  51, loss = 0.2876\n",
            "E:  19, B:  56, loss = 0.3675\n",
            "E:  19, B:  61, loss = 0.2707\n",
            "E:  19, B:  66, loss = 0.3807\n",
            "E:  20, B:   0, loss = 0.4385\n",
            "E:  20, B:   5, loss = 0.6348\n",
            "E:  20, B:  10, loss = 0.9746\n",
            "E:  20, B:  15, loss = 0.5756\n",
            "E:  20, B:  20, loss = 0.5508\n",
            "E:  20, B:  25, loss = 0.4653\n",
            "E:  20, B:  30, loss = 0.3473\n",
            "E:  20, B:  35, loss = 0.5071\n",
            "E:  20, B:  40, loss = 0.5896\n",
            "E:  20, B:  45, loss = 0.5860\n",
            "E:  20, B:  50, loss = 0.5563\n",
            "E:  20, B:  55, loss = 0.4353\n",
            "E:  20, B:  60, loss = 0.3793\n",
            "E:  20, B:  65, loss = 0.4304\n",
            "E:  21, B:   4, loss = 0.4410\n",
            "E:  21, B:   9, loss = 0.6202\n",
            "E:  21, B:  14, loss = 0.4194\n",
            "E:  21, B:  19, loss = 0.6404\n",
            "E:  21, B:  24, loss = 0.4878\n",
            "E:  21, B:  29, loss = 0.4155\n",
            "E:  21, B:  34, loss = 0.7297\n",
            "E:  21, B:  39, loss = 0.4540\n",
            "E:  21, B:  44, loss = 0.5418\n",
            "E:  21, B:  49, loss = 0.7834\n",
            "E:  21, B:  54, loss = 0.4845\n",
            "E:  21, B:  59, loss = 0.2630\n",
            "E:  21, B:  64, loss = 0.6842\n",
            "E:  22, B:   3, loss = 0.5676\n",
            "E:  22, B:   8, loss = 0.4210\n",
            "E:  22, B:  13, loss = 0.3930\n",
            "E:  22, B:  18, loss = 0.6018\n",
            "E:  22, B:  23, loss = 0.3461\n",
            "E:  22, B:  28, loss = 0.4433\n",
            "E:  22, B:  33, loss = 0.6760\n",
            "E:  22, B:  38, loss = 0.4300\n",
            "E:  22, B:  43, loss = 0.3958\n",
            "E:  22, B:  48, loss = 0.4926\n",
            "E:  22, B:  53, loss = 0.3793\n",
            "E:  22, B:  58, loss = 0.3850\n",
            "E:  22, B:  63, loss = 0.4479\n",
            "E:  23, B:   2, loss = 0.7536\n",
            "E:  23, B:   7, loss = 0.5208\n",
            "E:  23, B:  12, loss = 0.4068\n",
            "E:  23, B:  17, loss = 0.6121\n",
            "E:  23, B:  22, loss = 0.3867\n",
            "E:  23, B:  27, loss = 0.4832\n",
            "E:  23, B:  32, loss = 0.3623\n",
            "E:  23, B:  37, loss = 0.6729\n",
            "E:  23, B:  42, loss = 0.7120\n",
            "E:  23, B:  47, loss = 0.7014\n",
            "E:  23, B:  52, loss = 0.4893\n",
            "E:  23, B:  57, loss = 0.4965\n",
            "E:  23, B:  62, loss = 0.6817\n",
            "E:  23, B:  67, loss = 0.1003\n",
            "E:  24, B:   1, loss = 0.6715\n",
            "E:  24, B:   6, loss = 0.4986\n",
            "E:  24, B:  11, loss = 0.4001\n",
            "E:  24, B:  16, loss = 0.4094\n",
            "E:  24, B:  21, loss = 0.5246\n",
            "E:  24, B:  26, loss = 0.6619\n",
            "E:  24, B:  31, loss = 0.4725\n",
            "E:  24, B:  36, loss = 0.6779\n",
            "E:  24, B:  41, loss = 0.8622\n",
            "E:  24, B:  46, loss = 0.6425\n",
            "E:  24, B:  51, loss = 0.3126\n",
            "E:  24, B:  56, loss = 0.3500\n",
            "E:  24, B:  61, loss = 0.2627\n",
            "E:  24, B:  66, loss = 0.3717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1c0a546"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Just evaluate on the testing set"
      ],
      "id": "e1c0a546"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19766d85"
      },
      "source": [
        "# Convert the testing samples to tensor\n",
        "xTest = torch.tensor(xTest)\n",
        "\n",
        "# Compute the output\n",
        "with torch.no_grad():\n",
        "    yHat = GNN(xTest)\n",
        "\n",
        "yHat = yHat.detach().cpu().numpy() # Convert to numpy"
      ],
      "id": "19766d85",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb1a77cf",
        "outputId": "e451111e-c3fb-40f6-b02f-f75ea07d0a72"
      },
      "source": [
        "# Calculate the error\n",
        "yHat = np.argmax(yHat, axis = 1) # Take the maximum of each class\n",
        "print(\"Classification error: %.3f%%\" % (np.sum(np.abs(yHat - yTest))/nTest*100))"
      ],
      "id": "bb1a77cf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification error: 24.359%\n"
          ]
        }
      ]
    }
  ]
}